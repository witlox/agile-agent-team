# Agile Agent Team Experiment Configuration

experiment:
  name: "baseline-experiment"
  sprint_duration_minutes: 20
  sprints_per_stakeholder_review: 5

team:
  config_dir: "team_config"

  # Team size constraints
  max_engineers: 10  # Excluding testers - quality over quantity
  max_total_team_size: 13  # Including testers, PO, leads

  # Turnover simulation (for experiments > 5 months / 10 sprints)
  turnover:
    enabled: false  # Set true to simulate team member departures
    starts_after_sprint: 10  # ~5 months
    probability_per_sprint: 0.05  # 5% chance per sprint after threshold
    backfill_enabled: true  # Automatically hire replacement (simulated)

  # Tester participation in pairing
  tester_pairing:
    enabled: true  # Testers can join pairing sessions as navigators
    frequency: 0.20  # 20% of pairing sessions include a tester
    role: "navigator"  # Testers always navigate, never drive

  wip_limits:
    in_progress: 4
    review: 2

  quality_gates:
    min_test_coverage_lines: 85
    min_test_coverage_branches: 80
    max_cyclomatic_complexity: 10

  definition_of_done:
    - "All acceptance criteria met"
    - "Test coverage >= 85%"
    - "All tests passing"
    - "Code reviewed by pair"
    - "Deployed to staging"
    - "PO acceptance"

disturbances:
  enabled: true

  frequencies:
    dependency_breaks: 0.166  # 1 in 6 sprints
    production_incident: 0.125  # 1 in 8 sprints
    flaky_test: 0.25  # 1 in 4 sprints
    scope_creep: 0.20  # 1 in 5 sprints
    junior_misunderstanding: 0.33  # 1 in 3 sprints
    architectural_debt_surfaces: 0.166  # 1 in 6 sprints
    merge_conflict: 0.30  # 1 in 3 sprints (expected with gitflow)

  blast_radius_controls:
    max_velocity_impact: 0.30  # Max 30% velocity drop
    max_quality_regression: 0.15  # Max 15% coverage drop

models:
  vllm_endpoint: "http://vllm-gh200-module-1:8000"

# Runtime configurations for tool-using agents
runtimes:
  # Anthropic (Claude) runtime - requires API key and internet
  anthropic:
    enabled: true
    api_key_env: "ANTHROPIC_API_KEY"
    default_model: "claude-sonnet-4-5"
    fallback_model: "claude-opus-4-6"
    max_tokens: 8192

  # Local vLLM runtime - fully offline, uses self-hosted models
  local_vllm:
    enabled: true
    endpoint: "http://vllm-gh200-module-1:8000"
    tool_use_protocol: "xml"  # XML-based tool calling
    max_tokens: 8192
    temperature: 0.7

  # Shared tool configuration
  tools:
    workspace_root: "/tmp/agent-workspace"  # Base directory for code generation
    allowed_commands:
      - "git"
      - "pytest"
      - "python"
      - "pip"
      - "mypy"
      - "black"
      - "ruff"
      - "npm"
      - "node"
      - "ls"
      - "cat"
      - "grep"
      - "find"
      - "mkdir"
      - "echo"
    blocked_patterns:
      - "rm -rf /"
      - "dd if="
      - "mkfs"
      - "sudo"
      - "curl.*|.*bash"
      - "wget.*|.*sh"

# Code generation and workspace configuration
code_generation:
  # Workspace mode: how workspaces are organized
  workspace_mode: "per_story"  # Options: per_story | per_sprint
  # per_story: Each story gets isolated workspace (default)
  # per_sprint: All stories in sprint share one workspace

  # Cross-sprint persistence
  persist_across_sprints: false  # If true, sprint N+1 continues from sprint N's main
  merge_completed_stories: false  # Auto-merge feature branches to main after QA approval

  # Repository configuration (for brownfield development)
  repo_config:
    url: ""  # Git repo URL (leave empty for greenfield)
    branch: "main"  # Base branch to work from
    clone_mode: "fresh"  # Options: fresh | incremental
    # fresh: Delete and re-clone each time (greenfield default)
    # incremental: Reuse workspace, pull latest (brownfield)

  # Code coverage configuration
  coverage:
    enabled: true  # Collect real code coverage with pytest-cov
    source: "src"  # Directory to measure coverage for
    min_line_coverage: 85  # Minimum line coverage for Definition of Done
    min_branch_coverage: 80  # Minimum branch coverage for Definition of Done

# Agent definitions using new compositional structure
# Each agent = individual personality + seniority + specialization(s) + role archetype
#
# Team Structure:
# - Core Team: 11 agents (permanent team members)
# - Language Specialists: 5 agents (for Sprint 0 infrastructure setup)
# - External Consultants: 5 domains (on-demand via SpecialistConsultantSystem)
#
# Total: 16 agents defined in config + 5 external consultant domains
models:
  agents:
    # =========================================================================
    # CORE TEAM (11 agents)
    # =========================================================================
    # The permanent team that works on all sprints.
    # Composition: 2 Senior Devs, 2 Mid Devs, 2 Junior Devs, 2 Testers,
    #              1 QA Lead, 1 Dev Lead, 1 Product Owner
    # =========================================================================

    # --- Senior Developers (2) ---
    alex_senior_networking:
      name: "Alex Chen (Senior Networking Specialist)"
      individual: alex_chen
      seniority: senior
      specializations: [networking, security]
      role_archetype: developer+leader
      demographics:
        pronouns: "he/him"
        cultural_background: "Chinese-American"
      runtime: "local_vllm"  # Can use: local_vllm | anthropic
      tools: ["filesystem", "git", "bash"]  # Available tools
      model: "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
      temperature: 0.7
      max_tokens: 3072

    priya_senior_devops:
      name: "Priya Sharma (Senior DevOps Specialist)"
      individual: priya_sharma
      seniority: senior
      specializations: [devops, cloud_architecture]
      role_archetype: developer
      demographics:
        pronouns: "she/her"
        cultural_background: "Indian"
      runtime: "local_vllm"
      tools: ["filesystem", "git", "bash"]
      model: "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
      temperature: 0.7
      max_tokens: 3072

    # --- Mid-Level Developers (2) ---
    marcus_mid_backend:
      name: "Marcus Okafor (Mid-Level Backend Developer)"
      individual: marcus_okafor
      seniority: mid
      specializations: [backend, api_design]
      role_archetype: developer
      demographics:
        pronouns: "he/him"
        cultural_background: "Nigerian"
      runtime: "local_vllm"
      tools: ["filesystem", "git"]
      model: "Qwen/Qwen2.5-Coder-14B-Instruct"
      temperature: 0.7
      max_tokens: 2048

    elena_mid_frontend:
      name: "Elena Volkov (Mid-Level Frontend Developer)"
      individual: elena_volkov
      seniority: mid
      specializations: [frontend, ui_ux]
      role_archetype: developer
      demographics:
        pronouns: "she/her"
        cultural_background: "Russian"
      runtime: "local_vllm"
      tools: ["filesystem", "git"]
      model: "Qwen/Qwen2.5-Coder-14B-Instruct"
      temperature: 0.7
      max_tokens: 2048

    # --- Junior Developers (2) ---
    jamie_junior_fullstack:
      name: "Jamie Rodriguez (Junior Full-Stack Developer)"
      individual: jamie_rodriguez
      seniority: junior
      specializations: [backend, frontend]
      role_archetype: developer
      demographics:
        pronouns: "they/them"
        cultural_background: "Mexican-American"
      runtime: "local_vllm"
      tools: ["filesystem"]  # Limited tools for juniors
      model: "Qwen/Qwen2.5-Coder-7B-Instruct"
      temperature: 0.8
      max_tokens: 2048

    jordan_junior_backend:
      name: "Jordan Kim (Junior Backend Developer)"
      individual: jordan_kim
      seniority: junior
      specializations: [backend, database]
      role_archetype: developer
      demographics:
        pronouns: "he/him"
        cultural_background: "Korean-American"
      runtime: "local_vllm"
      tools: ["filesystem"]
      model: "Qwen/Qwen2.5-Coder-7B-Instruct"
      temperature: 0.8
      max_tokens: 2048

    # --- Testers (2 + 1 QA Lead) ---
    yuki_senior_tester_integration:
      name: "Yuki Tanaka (Senior Integration Tester)"
      individual: yuki_tanaka
      seniority: senior
      specializations: [test_automation, backend]
      role_archetype: tester
      demographics:
        pronouns: "they/them"
        cultural_background: "Japanese"
      runtime: "local_vllm"
      tools: ["filesystem", "bash"]  # Testers need to run tests
      model: "Qwen/Qwen2.5-14B-Instruct"
      temperature: 0.7
      max_tokens: 2048

    maria_mid_tester_e2e:
      name: "Maria Santos (Mid-Level E2E Tester)"
      individual: maria_santos
      seniority: mid
      specializations: [test_automation, frontend]
      role_archetype: tester
      demographics:
        pronouns: "she/her"
        cultural_background: "Brazilian"
      runtime: "local_vllm"
      tools: ["filesystem", "bash"]
      model: "Qwen/Qwen2.5-14B-Instruct"
      temperature: 0.7
      max_tokens: 2048

    sophie_senior_qa_lead:
      name: "Sophie Dubois (QA Lead)"
      individual: sophie_dubois
      seniority: senior
      specializations: [test_automation, performance_optimization]
      role_archetype: tester+leader
      demographics:
        pronouns: "she/her"
        cultural_background: "French"
      runtime: "local_vllm"
      tools: ["filesystem", "git", "bash"]
      model: "Qwen/Qwen2.5-72B-Instruct"
      temperature: 0.7
      max_tokens: 4096

    # --- Leaders (Dev Lead + Product Owner) ---
    ahmed_senior_dev_lead:
      name: "Ahmed Hassan (Development Lead)"
      individual: ahmed_hassan
      seniority: senior
      specializations: [backend, distributed_systems]
      role_archetype: developer+leader
      demographics:
        pronouns: "he/him"
        cultural_background: "Egyptian"
      runtime: "local_vllm"
      tools: ["filesystem", "git", "bash"]
      model: "Qwen/Qwen2.5-Coder-32B-Instruct"
      temperature: 0.7
      max_tokens: 4096

    alex_senior_po:
      name: "Alex Chen (Product Owner)"
      individual: alex_chen  # Reusing personality (allowed)
      seniority: senior
      specializations: [api_design]
      role_archetype: leader
      demographics:
        pronouns: "he/him"
        cultural_background: "Chinese-American"
      runtime: "local_vllm"
      tools: ["filesystem"]  # PO only needs to read, not write
      model: "Qwen/Qwen2.5-72B-Instruct"
      temperature: 0.8
      max_tokens: 4096

    # =========================================================================
    # LANGUAGE SPECIALISTS (5 agents)
    # =========================================================================
    # Specialized developers for Sprint 0 multi-language infrastructure setup.
    # They set up CI/CD, linters, formatters, and testing frameworks for each
    # language. After Sprint 0, they can contribute to regular development.
    # =========================================================================

    liam_senior_python:
      name: "Liam O'Brien (Senior Python Specialist)"
      individual: liam_obrien
      seniority: senior
      specializations: [python_specialist, backend]
      role_archetype: developer
      demographics:
        pronouns: "he/him"
        cultural_background: "Irish"
      runtime: "local_vllm"
      tools: ["filesystem", "git", "bash", "format_code", "lint_code", "run_tests"]
      model: "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
      temperature: 0.7
      max_tokens: 3072

    maya_senior_golang:
      name: "Maya Patel (Senior Go Specialist)"
      individual: maya_patel
      seniority: senior
      specializations: [golang_specialist, backend]
      role_archetype: developer
      demographics:
        pronouns: "she/her"
        cultural_background: "Indian-American"
      runtime: "local_vllm"
      tools: ["filesystem", "git", "bash", "format_code", "lint_code", "run_tests", "build_code"]
      model: "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
      temperature: 0.7
      max_tokens: 3072

    kai_senior_rust:
      name: "Kai Anderson (Senior Rust Specialist)"
      individual: kai_anderson
      seniority: senior
      specializations: [rust_specialist, systems_programming]
      role_archetype: developer
      demographics:
        pronouns: "they/them"
        cultural_background: "Swedish-American"
      runtime: "local_vllm"
      tools: ["filesystem", "git", "bash", "format_code", "lint_code", "run_tests", "build_code"]
      model: "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
      temperature: 0.7
      max_tokens: 3072

    aria_senior_typescript:
      name: "Aria Cohen (Senior TypeScript Specialist)"
      individual: aria_cohen
      seniority: senior
      specializations: [typescript_specialist, frontend]
      role_archetype: developer
      demographics:
        pronouns: "she/her"
        cultural_background: "Israeli-American"
      runtime: "local_vllm"
      tools: ["filesystem", "git", "bash", "format_code", "lint_code", "run_tests", "build_code"]
      model: "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
      temperature: 0.7
      max_tokens: 3072

    dmitri_senior_cpp:
      name: "Dmitri Volkov (Senior C++ Specialist)"
      individual: dmitri_volkov
      seniority: senior
      specializations: [cpp_specialist, systems_programming]
      role_archetype: developer
      demographics:
        pronouns: "he/him"
        cultural_background: "Russian"
      runtime: "local_vllm"
      tools: ["filesystem", "git", "bash", "format_code", "lint_code", "run_tests", "build_code"]
      model: "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
      temperature: 0.7
      max_tokens: 3072

# =============================================================================
# EXTERNAL CONSULTANTS (5 domains - NOT in config, created on-demand)
# =============================================================================
# Managed by SpecialistConsultantSystem (src/orchestrator/specialist_consultant.py)
# Available domains (profiles in team_config/07_specialists/):
#   - ml: Machine Learning / AI (training, deployment, debugging)
#   - security: Auth, OWASP Top 10, secure coding
#   - performance: Optimization, profiling, benchmarking
#   - cloud: AWS, GCP, Azure, Kubernetes
#   - architecture: System design, scalability, patterns
#
# Constraints:
#   - Max 3 consultations per sprint (hard limit)
#   - Velocity penalty: 2.0 story points per consultation
#   - Only triggered when team lacks expertise for a blocker
#   - Specialist pairs with junior/mid for knowledge transfer (1 day)
#
# Usage: Automatically detected during daily standups when Dev Lead identifies
#        expertise gap blockers, or manually via SpecialistRequest.
# =============================================================================

database:
  url: "postgresql://postgres:password@shared-db:5432/team_context"
  redis_url: "redis://redis:6379"

monitoring:
  prometheus_port: 8080
  grafana_url: "http://grafana:3000"

profile_swapping:
  mode: "constrained"  # none | constrained | free

  allowed_scenarios:
    - "critical_production_incident"
    - "specialist_unavailable"
    - "deliberate_cross_training"

  penalties:
    context_switch_slowdown: 1.20  # 20% slower first task
    proficiency_reduction: 0.70  # 70% of true specialist
    knowledge_decay_sprints: 1  # Decays after 1 sprint if not used

# Remote Git Integration (GitHub/GitLab)
# Enables agents to push code and create/review pull requests
remote_git:
  enabled: false  # Set to true to enable remote push/PR workflow
  provider: "github"  # Options: github | gitlab

  # GitHub configuration (single service account + per-agent author attribution)
  github:
    token_env: "GITHUB_TOKEN"  # Environment variable containing GitHub token
    base_branch: "main"
    merge_method: "squash"  # Options: merge | squash | rebase
    draft_prs: false  # Create PRs as drafts initially

  # GitLab configuration (per-agent accounts for self-hosted instances)
  gitlab:
    token_env_pattern: "GITLAB_TOKEN_{role_id}"  # Pattern: GITLAB_TOKEN_alex_senior_networking
    base_branch: "main"
    merge_method: "squash"  # Options: merge | squash
    draft_prs: false

  # Git author metadata for commit attribution
  author_email_domain: "agent.local"  # Generates emails like alex_senior_networking@agent.local

# Sprint 0 Infrastructure Setup
sprint_zero:
  enabled: true  # Set to false to skip Sprint 0 and start with Sprint 1
