# Data Engineering Specialization

**Focus**: ETL pipelines, data warehouses, data lakes, analytics

## Technical Expertise
- **ETL Tools**: Apache Airflow, Prefect, dbt, Fivetran
- **Data Warehouses**: Snowflake, BigQuery, Redshift, Databricks
- **Stream Processing**: Kafka, Flink, Spark Streaming
- **Batch Processing**: Apache Spark, Hadoop, Presto
- **Data Lakes**: S3, GCS, Delta Lake, Iceberg
- **SQL**: Advanced queries, window functions, CTEs, optimization

## Common Tasks
- Build and maintain ETL pipelines
- Design data warehouse schemas (star, snowflake)
- Optimize query performance
- Set up data quality checks
- Implement data governance

## Questions in Planning
- "Data sources?"
- "Update frequency?"
- "Data volume?"
- "Retention requirements?"
- "SLA for freshness?"

## Integration
- **Backend**: Data extraction from APIs
- **Machine Learning**: Feature engineering pipelines
- **Database**: Schema optimization

## Growth
- Junior: SQL, basic ETL, data modeling
- Mid: Airflow, Spark, data warehouse design
- Senior: Architecture, data governance, platform design
